{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b225e640-41fd-4a14-a15e-a2d98d6e8881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS + FixedAttention layer + custom loss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, layers\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Residual Attention Layer\n",
    "# ---------------------------------------------------\n",
    "class ResidualAttention(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ResidualAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = int(input_shape[-1])\n",
    "        self.att_logits = self.add_weight(\n",
    "            name=\"att_logits\",\n",
    "            shape=(d, d),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        A = tf.nn.softmax(self.att_logits, axis=-1)\n",
    "        Ax = tf.linalg.matmul(x, A, transpose_b=True)\n",
    "        return x + Ax   # <--- RESIDUAL CONNECTION\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Custom Loss\n",
    "# ---------------------------------------------------\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mag_loss = K.square(y_true[:, 0] - y_pred[:, 0])\n",
    "    cos_loss = K.square(y_true[:, 1] - y_pred[:, 1])\n",
    "    sin_loss = K.square(y_true[:, 2] - y_pred[:, 2])\n",
    "    return K.mean(2.0 * mag_loss + cos_loss + sin_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4c7ae-d0e3-42da-aa91-3178b51ca360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD & PREPARE DATA\n",
    "data = pd.read_csv(\"output_Rx_PEC_lossy_2layers_angles_all.csv\")\n",
    "\n",
    "X = data.iloc[:, :8].values\n",
    "y_mag = data[\"Zsmag\"].values\n",
    "y_phase = data[\"Zsphase\"].values  # must be rad\n",
    "\n",
    "y_log = np.log10(y_mag + 1e-9)\n",
    "y_cos = np.cos(y_phase)\n",
    "y_sin = np.sin(y_phase)\n",
    "\n",
    "y = np.column_stack((y_log, y_cos, y_sin))\n",
    "\n",
    "# Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.98, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "joblib.dump(scaler, \"input_lossy_2layers_Residual_Attention3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4acd37-431c-43aa-a0c5-ebf4da9af67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILD MODEL WITH FIXED ATTENTION\n",
    "inputs = Input(shape=(8,))\n",
    "\n",
    "# --- Residual Attention ---\n",
    "x = ResidualAttention()(inputs)\n",
    "\n",
    "# --- MLP ---\n",
    "x = Dense(512, activation='relu', kernel_initializer='he_normal')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = Dense(256, activation='relu', kernel_initializer='he_normal')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, activation='relu', kernel_initializer='he_normal')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "outputs = Dense(3, activation='linear')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss=custom_loss,\n",
    "    metrics=[\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57475b0b-a983-4cdf-9c70-18af524b4f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Adjusted callbacks for a huge dataset \n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,         # Wait for 15 epochs without significant improvement\n",
    "    min_delta=1e-4,      # Improvement threshold \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.7,          # Reduce learning rate by 20% instead of 30%\n",
    "    patience=5,         # Wait 10 epochs with no improvement before reducing LR\n",
    "    min_lr=1e-6,         # Lower bound for the learning rate\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=300,\n",
    "    batch_size=1024,     # Increase batch size for efficiency on a huge dataset\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "model.save(\"Zsurf_model_lossy_2layers_Residual_Attention3.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a9f514-69a4-4f66-b369-c2890c9c5a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test MAE :\", test_mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tfenv)",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
